#Import numpy to work with tweets as a DataFrame
import numpy as np 
import pandas as pd 

#Import matplotlib to plot graphs and wordcloud
import matplotlib.pyplot as plt 
import re
import time

#Import NLTK for sentiment analysis
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import *
from nltk.classify import NaiveBayesClassifier
from wordcloud import WordCloud

#Import Tweepy to connect and scrape tweets from Twitter's API
import tweepy
from tweepy import OAuthHandler 

# for showing all the plots inline
%matplotlib inline

##Getting User information from Tweets
#Setting up the code to pull and covert tweets to a CSV file
tweets = []

def text_query_to_csv(text_query,count):
    try:
        # Creation of query method using parameters
        tweets = tweepy.Cursor(api.search_tweets,q=text_query,tweet_mode='extended',lang='en',result_type='mixed').items(count) 
       
        # Pulling information from tweets iterable object
        tweets_list = [[tweet.id, tweet.created_at, tweet.full_text, tweet.user.screen_name, tweet.user.id,
                       tweet.user.location, tweet.user.description, tweet.user.followers_count] for tweet in tweets]
        print("Number of tweets extracted: {}.\n".format(len(tweets_list)))
        
        # Creation of dataframe from tweets list
        tweets_df = pd.DataFrame(tweets_list,columns=['Tweet Id', 'Tweet Datetime', 'Tweet Text', 'Twitter Handle',
                                                     'Twitter User ID', 'Twitter User Location', 'Twitter User Description',
                                                     'User Followers Count'])
        

        # Converting dataframe to CSV 
        tweets_df.to_csv('{}-tweets.csv'.format(text_query), sep=',', index = False)

    except BaseException as e:
        print('failed on_status,',str(e))
        time.sleep(3)
        

##Here is where we input the query, which takes a string input and count the number of tweets to pull
# Input search query to scrape tweets and name csv file
# Max recent tweets pulls x amount of most recent tweets from that user
text_query = 'Ukraine War'
count = 20000

# Calling function to query X amount of relevant tweets and create a CSV file
text_query_to_csv(text_query, count)


##Re-importing the CSV file as a DataFrame
#Reimport the CSV into a pandas DataFrame
df = pd.read_csv('20kUkraine war-tweets.csv')
df.head()

##Tweet Cleaning using NLTK and Regex, removing any http links, hashtags(but keeping the text) and @ signs
#Import the necessary libraries to analyse the text
words = set(nltk.corpus.words.words())

#Clean Tweets using Regex
def cleaner(tweet):
    tweet = re.sub("@[A-Za-z0-9]+","",tweet) #Remove @ sign
    tweet = re.sub(r"(?:\@|http?\://|https?\://|www)\S+", "", tweet) #Remove http links
    tweet = " ".join(tweet.split())
    tweet = tweet.replace("#", "").replace("_", " ") #Remove hashtag sign but keep the text
    tweet = " ".join(w for w in nltk.wordpunct_tokenize(tweet)
                     if w.lower() in words or not w.isalpha())
    return tweet
    
df['Clean Tweet'] = df['Tweet Text'].apply(cleaner)

#Remove tweets with empty text
df = df[df['Clean Tweet']!='']

#Drop duplicate rows
df.drop_duplicates(subset=['Clean Tweet'], keep=False)

#Reset index
df = df.reset_index(drop=True)
df.head()


##Using NLTK for Sentiment Analysis
#Analyse Sentiment
sid = SentimentIntensityAnalyzer()
list1 = []
for i in df['Clean Tweet']:
    list1.append((sid.polarity_scores(str(i)))['compound'])

#Convert the compound scores into categories - positive, negative and neutral
df['sentiment'] = pd.Series(list1)
def sentiment_category(sentiment):
    label = ''
    if(sentiment>0):
        label = 'positive'
    elif(sentiment == 0):
        label = 'neutral'
    else:
        label = 'negative'
    return(label)
df['sentiment_category'] = df['sentiment'].apply(sentiment_category)
df.head()


##Grouping tweets
#Group positive and negative sentiment and count them by date
neg = df[df['sentiment_category']=='negative']
neg = neg.groupby(['Tweet Datetime'],as_index=False).count()
pos = df[df['sentiment_category']=='positive']
pos = pos.groupby(['Tweet Datetime'],as_index=False).count()
pos = pos[['Tweet Datetime','Tweet Id']]
neg = neg[['Tweet Datetime','Tweet Id']]

#Visualise sentiment by date using Plotly
import plotly.graph_objs as go
fig = go.Figure()
for col in pos.columns:
    fig.add_trace(go.Scatter(x=pos['Tweet Datetime'], y=pos['Tweet Id'],
                             name = col,
                             mode = 'markers+lines',
                             line=dict(shape='linear'),
                             connectgaps=True,
                             line_color='green'
                             )
                 )
for col in neg.columns:
    fig.add_trace(go.Scatter(x=neg['Tweet Datetime'], y=neg['Tweet Id'],
                             name = col,
                             mode = 'markers+lines',
                             line=dict(shape='linear'),
                             connectgaps=True,
                             line_color='red'
                             )
                 )
fig.show()


##WordCloud
#Plotting prominent positive words
import matplotlib.pyplot as plt
from wordcloud import WordCloud
positive = df[df['sentiment_category']=='positive']
wordcloud = WordCloud(max_font_size=50, max_words=500, background_color="white").generate(str(positive['Clean Tweet']))
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

#Plotting prominent negative words
positive = df[df['sentiment_category']=='negative']
wordcloud = WordCloud(max_font_size=50, max_words=500, background_color="white").generate(str(positive['Clean Tweet']))
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()


##Loading the results to a SQL Database
#Importing the necessary libraries
import sqlalchemy as db 
from sqlalchemy import MetaData
meta = MetaData()
from sqlalchemy import Table, Column, Integer, String,MetaData
meta = MetaData()
from sqlalchemy.schema import CreateTable
import psycopg2

# Create connection engine
engine = db.create_engine('postgresql://postgres:admin@localhost:5432/project2') 
#user postgres, password admin - change yours accordingly. Also check the port number
conn = engine.raw_connection()


##To create tables holding the following informations: User_id, id, handle, user_description, user_location
commands = (# TABLE 1: user_info
            '''CREATE TABLE IF NOT EXISTS user_info(
twitter_user_id varchar(500), 
tweet_id varchar(500), 
twitter_handle varchar(500),
twitter_user_description varchar(2000),
twitter_user_location varchar(500));''')

# Initialize connection to PostgreSQL
cur = conn.cursor()
table_count = 0

# Create cursor to execute SQL commands
cur.execute(commands)
table_count += 1

# Close communication with server
conn.commit()
cur.close()
conn.close()

print(str(table_count),"table(s) have been created in PostgreSQL.")

##Loading information from CSV to SQL
##We had to pre-split the information we needed into CSV files
df=pd.read_csv(r'C:\Users\user\Desktop\juypter\interim project\Assignment\user_info.csv')
df.to_sql(name= 'user_info', con= engine, if_exists= 'append', index= False)
pd.read_sql("SELECT * FROM user_info", conn)

#Repeat the same steps to create and load information into Tables 2 and 3
